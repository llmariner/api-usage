syntax = "proto3";

package llmariner.apiusage.server.v1;

option go_package = "github.com/llmariner/api-usage/api/v1";

message UsageRecord {
  // user ID is the internal user ID.
  string user_id = 1;
  string tenant = 2;
  string organization = 3;
  string project = 4;

  string api_key_id = 10;

  string api_method = 5;
  int32 status_code = 6;

  int64 timestamp = 7;
  int32 latency_ms = 8;

  UsageDetails details = 9;

  // This measures the model's internal processing time, excluding network latency
  // (i.e., time between inference-manager-engine and inference runtime).
  int32 runtime_latency_ms = 11;

  // Next ID: 12
}

message UsageDetails {
  oneof message {
    CreateChatCompletion create_chat_completion = 1;
    CreateCompletion create_completion = 2;
    CreateAudioTranscription create_audio_transcription = 3;
    CreateModelResponse create_model_response = 4;
    Tokenize tokenize = 5;
  }
}

message CreateChatCompletion {
  string model_id = 1;
  // How quickly users start seeing the model's output after entering their query.
  int32 time_to_first_token_ms = 2;
  // The number off tokens in the prompt.
  int32 prompt_tokens = 3;
  // The number of tokens in the completion.
  int32 completion_tokens = 4;

  // How quickly users start seeing the model's output after entering their query.
  // This measures the model's internal processing time, excluding network latency
  // (i.e., time between inference-manager-engine and inference runtime).
  int32 runtime_time_to_first_token_ms = 5;
}

message CreateCompletion {
  string model_id = 1;
  // How quickly users start seeing the model's output after entering their query.
  int32 time_to_first_token_ms = 2;
  // The number off tokens in the prompt.
  int32 prompt_tokens = 3;
  // The number of tokens in the completion.
  int32 completion_tokens = 4;

  // How quickly users start seeing the model's output after entering their query.
  // This measures the model's internal processing time, excluding network latency
  // (i.e., time between inference-manager-engine and inference runtime).
  int32 runtime_time_to_first_token_ms = 5;
}

message CreateAudioTranscription {
  string model_id = 1;

  // How quickly users start seeing the model's output after entering their query.
  int32 time_to_first_token_ms = 2;

  int32 input_tokens = 3;
  int32 output_tokens = 4;
  int32 total_tokens = 5;

  int32 text_tokens = 6;
  int32 audio_tokens = 7;

  double input_duration_seconds = 8;

  // How quickly users start seeing the model's output after entering their query.
  // This measures the model's internal processing time, excluding network latency
  // (i.e., time between inference-manager-engine and inference runtime).
  int32 runtime_time_to_first_token_ms = 9;
}

message CreateModelResponse {
  string model_id = 1;

  // How quickly users start seeing the model's output after entering their query.
  int32 time_to_first_token_ms = 2;

  // How quickly users start seeing the model's output after entering their query.
  // This measures the model's internal processing time, excluding network latency
  // (i.e., time between inference-manager-engine and inference runtime).
  int32 runtime_time_to_first_token_ms = 3;

  // TODO(kenji): Add more.
}

message Tokenize {
  string model_id = 1;
}

message CreateUsageRequest {
  repeated UsageRecord records = 1;
}

message Usage {}

service CollectionInternalService {
  rpc CreateUsage(CreateUsageRequest) returns (Usage);
}
